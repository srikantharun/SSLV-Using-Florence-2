<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Encoder Architecture Comparison</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        h1 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 30px;
        }
        
        .container {
            display: flex;
            flex-direction: column;
            gap: 30px;
        }
        
        .architecture-section {
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            background-color: #fff;
            transition: transform 0.3s ease;
        }
        
        .architecture-section:hover {
            transform: translateY(-5px);
        }
        
        .architecture-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
            border-bottom: 2px solid #f0f0f0;
            padding-bottom: 10px;
        }
        
        .architecture-title {
            font-size: 1.8em;
            color: #3498db;
            margin: 0;
        }
        
        .architecture-year {
            background-color: #3498db;
            color: white;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.8em;
        }
        
        .architecture-content {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        
        .visualization {
            position: relative;
            height: 400px;
            border: 1px solid #e0e0e0;
            border-radius: 4px;
            overflow: hidden;
        }
        
        .key-features {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 4px;
        }
        
        .key-features h3 {
            margin-top: 0;
            color: #2c3e50;
        }
        
        .key-features ul {
            margin: 0;
            padding-left: 20px;
        }
        
        .key-features li {
            margin-bottom: 8px;
        }
        
        .pros-cons {
            display: flex;
            gap: 20px;
        }
        
        .pros, .cons {
            flex: 1;
            padding: 15px;
            border-radius: 4px;
        }
        
        .pros {
            background-color: #e8f8f5;
        }
        
        .cons {
            background-color: #fdedec;
        }
        
        .pros h3, .cons h3 {
            margin-top: 0;
        }
        
        .pros h3 {
            color: #27ae60;
        }
        
        .cons h3 {
            color: #e74c3c;
        }
        
        svg {
            width: 100%;
            height: 100%;
        }
        
        .tabs {
            display: flex;
            margin-bottom: 20px;
        }
        
        .tab {
            padding: 10px 20px;
            background-color: #f0f0f0;
            cursor: pointer;
            border: 1px solid #ddd;
            border-bottom: none;
            border-radius: 5px 5px 0 0;
            margin-right: 5px;
        }
        
        .tab.active {
            background-color: #fff;
            font-weight: bold;
            border-bottom: 1px solid #fff;
            margin-bottom: -1px;
            z-index: 1;
        }
        
        .tab-content {
            display: none;
        }
        
        .tab-content.active {
            display: block;
        }
        
        .highlight {
            font-weight: bold;
            color: #e67e22;
        }
        
        .architecture-flow {
            display: flex;
            align-items: center;
            justify-content: space-between;
            width: 100%;
            height: 100%;
            padding: 20px;
        }
        
        .component {
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
            padding: 15px;
            border: 2px solid #3498db;
            border-radius: 8px;
            background-color: #f7fbfe;
            position: relative;
            min-width: 100px;
            margin: 0 5px;
        }
        
        .component-title {
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        .arrow {
            width: 50px;
            height: 2px;
            background-color: #95a5a6;
            position: relative;
        }
        
        .arrow::after {
            content: '';
            position: absolute;
            right: 0;
            top: -4px;
            border-style: solid;
            border-width: 5px 0 5px 10px;
            border-color: transparent transparent transparent #95a5a6;
        }
        
        .compare-button {
            display: block;
            margin: 20px auto;
            padding: 12px 25px;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 4px;
            font-size: 1em;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        
        .compare-button:hover {
            background-color: #2980b9;
        }
        
        .compare-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
            display: none;
        }
        
        .compare-table th, .compare-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        .compare-table th {
            background-color: #f2f2f2;
        }
        
        .compare-table tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        .compare-table tr:hover {
            background-color: #f5f5f5;
        }
    </style>
</head>
<body>
    <h1>Image Encoder Architectures: CNN vs ViT vs DaViT</h1>
    
    <div class="tabs">
        <div class="tab active" onclick="switchTab('cnn')">Traditional CNN</div>
        <div class="tab" onclick="switchTab('vit')">Vision Transformer (ViT)</div>
        <div class="tab" onclick="switchTab('davit')">DaViT</div>
    </div>
    
    <div class="container">
        <div id="cnn" class="tab-content active">
            <div class="architecture-section">
                <div class="architecture-header">
                    <h2 class="architecture-title">Traditional CNN Architecture</h2>
                    <span class="architecture-year">~2012-Present</span>
                </div>
                <div class="architecture-content">
                    <div class="visualization">
                        <div class="architecture-flow">
                            <div class="component">
                                <div class="component-title">Input Image</div>
                                <div>224×224×3</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Conv Layers</div>
                                <div>Local Features</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Pooling</div>
                                <div>Downsampling</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Feature Maps</div>
                                <div>Hierarchical</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Fully Connected</div>
                                <div>Classification</div>
                            </div>
                        </div>
                    </div>
                    <div class="key-features">
                        <h3>Key Characteristics</h3>
                        <ul>
                            <li><span class="highlight">Local Processing</span>: Uses convolutional kernels that operate on small spatial regions</li>
                            <li><span class="highlight">Weight Sharing</span>: Applies the same filters across the entire image</li>
                            <li><span class="highlight">Hierarchical Structure</span>: Early layers detect simple features (edges, textures), deeper layers combine them into complex concepts</li>
                            <li><span class="highlight">Spatial Reduction</span>: Uses pooling layers to progressively reduce spatial dimensions</li>
                            <li><span class="highlight">Inductive Bias</span>: Built-in assumptions about locality and translation invariance</li>
                        </ul>
                    </div>
                    <div class="pros-cons">
                        <div class="pros">
                            <h3>Advantages</h3>
                            <ul>
                                <li>Parameter efficient due to weight sharing</li>
                                <li>Works well with limited data</li>
                                <li>Computationally efficient</li>
                                <li>Strong inductive bias for image data</li>
                                <li>Well-understood architecture</li>
                            </ul>
                        </div>
                        <div class="cons">
                            <h3>Limitations</h3>
                            <ul>
                                <li>Limited receptive field (struggles with global context)</li>
                                <li>Fixed geometric priors</li>
                                <li>Difficulty capturing long-range dependencies</li>
                                <li>Limited attention to spatial relationships</li>
                                <li>Less flexible for multi-modal inputs</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div id="vit" class="tab-content">
            <div class="architecture-section">
                <div class="architecture-header">
                    <h2 class="architecture-title">Vision Transformer (ViT) Architecture</h2>
                    <span class="architecture-year">2020-Present</span>
                </div>
                <div class="architecture-content">
                    <div class="visualization">
                        <div class="architecture-flow">
                            <div class="component">
                                <div class="component-title">Input Image</div>
                                <div>224×224×3</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Patch Embedding</div>
                                <div>16×16 patches</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Position Embedding</div>
                                <div>Spatial info</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Self-Attention</div>
                                <div>Global context</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">MLP Head</div>
                                <div>Classification</div>
                            </div>
                        </div>
                    </div>
                    <div class="key-features">
                        <h3>Key Characteristics</h3>
                        <ul>
                            <li><span class="highlight">Patch-based Processing</span>: Divides images into fixed-size patches treated as tokens</li>
                            <li><span class="highlight">Self-Attention Mechanism</span>: Each patch can attend to every other patch</li>
                            <li><span class="highlight">Position Embeddings</span>: Adds learned position information to maintain spatial relationships</li>
                            <li><span class="highlight">Flat Architecture</span>: Processes all patches at the same resolution level</li>
                            <li><span class="highlight">No Inherent Inductive Bias</span>: Learns spatial relationships purely from data</li>
                        </ul>
                    </div>
                    <div class="pros-cons">
                        <div class="pros">
                            <h3>Advantages</h3>
                            <ul>
                                <li>Excellent at capturing global dependencies</li>
                                <li>More flexible representations</li>
                                <li>Scalable with more parameters and data</li>
                                <li>Transfers well to other tasks</li>
                                <li>Seamless integration with language models</li>
                            </ul>
                        </div>
                        <div class="cons">
                            <h3>Limitations</h3>
                            <ul>
                                <li>Computationally expensive (quadratic complexity)</li>
                                <li>Requires large datasets to train effectively</li>
                                <li>Limited local feature extraction</li>
                                <li>Higher parameter count</li>
                                <li>Fixed patch size regardless of image content</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div id="davit" class="tab-content">
            <div class="architecture-section">
                <div class="architecture-header">
                    <h2 class="architecture-title">DaViT Architecture (Florence-2)</h2>
                    <span class="architecture-year">2023-Present</span>
                </div>
                <div class="architecture-content">
                    <div class="visualization">
                        <div class="architecture-flow">
                            <div class="component">
                                <div class="component-title">Input Image</div>
                                <div>224×224×3</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Multi-scale Patches</div>
                                <div>Hierarchical</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Dual Attention</div>
                                <div>Window + Channel</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Pyramid Features</div>
                                <div>Multi-resolution</div>
                            </div>
                            <div class="arrow"></div>
                            <div class="component">
                                <div class="component-title">Vision Tokens</div>
                                <div>Flattened Embedding</div>
                            </div>
                        </div>
                    </div>
                    <div class="key-features">
                        <h3>Key Characteristics</h3>
                        <ul>
                            <li><span class="highlight">Hierarchical Design</span>: Processes images at multiple scales like CNNs</li>
                            <li><span class="highlight">Dual Attention Mechanism</span>: Combines window attention (local) and channel attention (global)</li>
                            <li><span class="highlight">Data Efficiency</span>: Requires less training data than standard ViTs</li>
                            <li><span class="highlight">Pyramid Structure</span>: Creates multi-resolution feature maps</li>
                            <li><span class="highlight">Hybrid Architecture</span>: Combines strengths of CNNs and Transformers</li>
                        </ul>
                    </div>
                    <div class="pros-cons">
                        <div class="pros">
                            <h3>Advantages</h3>
                            <ul>
                                <li>Better computational efficiency than ViT</li>
                                <li>Superior performance with less training data</li>
                                <li>Captures both local and global context</li>
                                <li>Better scaling properties</li>
                                <li>Ideal for multimodal tasks (like in Florence-2)</li>
                            </ul>
                        </div>
                        <div class="cons">
                            <h3>Limitations</h3>
                            <ul>
                                <li>More complex architecture to implement</li>
                                <li>Still computationally intensive compared to CNNs</li>
                                <li>Newer approach with less established best practices</li>
                                <li>May not be needed for simpler vision tasks</li>
                                <li>More hyperparameters to tune</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <button class="compare-button" onclick="toggleComparisonTable()">Show Side-by-Side Comparison</button>
    
    <table id="comparison-table" class="compare-table">
        <tr>
            <th>Feature</th>
            <th>Traditional CNN</th>
            <th>Vision Transformer (ViT)</th>
            <th>DaViT (Florence-2)</th>
        </tr>
        <tr>
            <td>Processing Style</td>
            <td>Local (sliding window)</td>
            <td>Global (all patches attend to each other)</td>
            <td>Multi-scale (hierarchical with dual attention)</td>
        </tr>
        <tr>
            <td>Feature Extraction</td>
            <td>Hierarchical, fixed kernels</td>
            <td>Flat, attention-based</td>
            <td>Hierarchical, attention-based</td>
        </tr>
        <tr>
            <td>Context Scope</td>
            <td>Limited by receptive field</td>
            <td>Global across entire image</td>
            <td>Both local and global via dual attention</td>
        </tr>
        <tr>
            <td>Parameter Efficiency</td>
            <td>High (shared weights)</td>
            <td>Low (many parameters)</td>
            <td>Medium (optimized attention)</td>
        </tr>
        <tr>
            <td>Data Requirements</td>
            <td>Works with smaller datasets</td>
            <td>Requires large datasets</td>
            <td>More data-efficient than ViT</td>
        </tr>
        <tr>
            <td>Computational Cost</td>
            <td>Lower (linear scaling)</td>
            <td>Higher (quadratic complexity)</td>
            <td>Medium (optimized attention)</td>
        </tr>
        <tr>
            <td>Inductive Bias</td>
            <td>Strong spatial bias</td>
            <td>Minimal inductive bias</td>
            <td>Moderate spatial bias</td>
        </tr>
        <tr>
            <td>Multimodal Integration</td>
            <td>Challenging</td>
            <td>Natural fit</td>
            <td>Optimized for multimodal tasks</td>
        </tr>
        <tr>
            <td>Output Representation</td>
            <td>Feature maps</td>
            <td>Token embeddings</td>
            <td>Flattened visual token embeddings</td>
        </tr>
    </table>
    
    <script>
        function switchTab(tabId) {
            // Hide all tab contents
            const contents = document.querySelectorAll('.tab-content');
            contents.forEach(content => {
                content.classList.remove('active');
            });
            
            // Deactivate all tabs
            const tabs = document.querySelectorAll('.tab');
            tabs.forEach(tab => {
                tab.classList.remove('active');
            });
            
            // Activate selected tab
            document.getElementById(tabId).classList.add('active');
            
            // Find and activate the corresponding tab button
            const tabIndex = ['cnn', 'vit', 'davit'].indexOf(tabId);
            tabs[tabIndex].classList.add('active');
        }
        
        function toggleComparisonTable() {
            const table = document.getElementById('comparison-table');
            const button = document.querySelector('.compare-button');
            
            if (table.style.display === 'table') {
                table.style.display = 'none';
                button.textContent = 'Show Side-by-Side Comparison';
            } else {
                table.style.display = 'table';
                button.textContent = 'Hide Comparison';
            }
        }
    </script>
</body>
</html>